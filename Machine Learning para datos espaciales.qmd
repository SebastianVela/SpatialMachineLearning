---
title: "Machine Learning para datos espaciales"
author: "Jhon Sebasti谩n Vela Salcedo"
format: 
  revealjs:
    theme: simple
    logo: Logo.jpg
    css: Ajustes.css
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
title-slide-attributes:
  data-background-size: contain
editor: visual
resources:
  - demo.pdf
---

# Introducci贸n {background-color="#581845"}

##  {style="font-size: 80%; "}

Varias 谩reas de aplicaci贸n usando datos espaciales como:

-   Uso del suelo y clasificaci贸n de la clasificaci贸n de la cubierta del suelo

-   Caracterizaci贸n transversal y cambio longitudinal

-   Crecimiento urbano

-   Agricultura y predicci贸n del rendimiento de los cultivos

-   Aparici贸n y propagaci贸n de enfermedades infecciosas

-   Transporte y an谩lisis de colisiones

-   Visualizaci贸n de mapas y cartograf铆a

-   Predicci贸n de trayectorias y patrones de movimiento

-   Clasificaci贸n de nubes de puntos

-   Interacci贸n espacial

-   Interpolaci贸n espacial y predicci贸n espaciotemporal

## Marco de referencia para la revisi贸n

![](Graficas/Fig.%201.1.png)

## B煤squeda de bibliograf铆a en Web of Science

![](Graficas/Fig.%201.2.png)

# Machine Learning {background-color="#581845"}

## Categor铆as

![](Graficas/Fig.%202.1.png)

------------------------------------------------------------------------

![](Graficas/Fig.%202.2.png)

# Propiedades de los datos espaciales {background-color="#581845"}

------------------------------------------------------------------------

### 1. Dependencia espacial

::: {style="font-size: 80%; "}
-   I de Moran:

$$I = \frac{\sum_i \sum_j W_{S_i S_j} (Z(S_i) - \overline Z) (Z(S_j) - \overline Z)}{\sigma^2(\sum_i \sum_j W_{ij})}$$

-   Semivariograma:

$$\hat \gamma(h) = \frac{1}{2d(h)} \sum_{|S_i - S_j|=h} (Z(S_i) - Z(S_j))^2$$ - Componentes m谩s complejas de dependencia espacial son el efecto de vecindad (Neighborhood effect) y el efecto de contagio (spillover effect)
:::

------------------------------------------------------------------------

::: {style="font-size: 80%; "}
### 2. Heterogeneidad espacial

-   Proceso no estacionario (media y varianza no son constantes)

-   Anisotrop铆a (la dependencia espacial es diferente en varias direcciones)

### 3. Escala

Existen dos retos cuando se trabaja con datos espaciales con unidades de 谩rea:

-   el problema de la unidad de 谩rea modificable (MAUP), est谩 relacionada con la sensibilidad de losresultados anal铆ticos a la definici贸n de las unidades geogr谩ficas para las que se recogen los datos

-   el problema del contexto geogr谩fico incierto (UGCoP). Se refiere a la sensibilidad de las variables contextuales y resultados anal铆ticos a las diferentes delimitaciones de las unidades contextuales
:::

# Machine Learning para datos espaciales {background-color="#581845"}

## 

Para llevar a cabo el aprendizaje autom谩tico de datos espaciales, necesitamos a帽adir relaciones de localizaci贸n, distancia o relaciones topol贸gicas al proceso de aprendizaje.

El proceso de aprendizaje es el siguiente:

![](Graficas/Fig.%203.1.png){fig-align="center"}

## 1. Matriz de observaciones espaciales

::: {style="font-size: 80%;"}
Una forma t铆pica de incluir propiedades espaciales en ML es encontrar una representaci贸n de esas propiedades en la matriz de observaciones X.

Varios aspectos cr铆ticos intervienen en la creaci贸n de una matriz de observaci贸n espacial utilizada como entrada para el algoritmo de aprendizaje:

1.1. El muestreo espacial

1.2. Las caracter铆sticas espaciales

1.3. La reducci贸n de la dimensionalidad

1.4. El tratamiento de los datos faltantes
:::

------------------------------------------------------------------------

### 1.1. El muestreo espacial

::: {style="font-size: 80%; "}
Dos puntos importantes:

-   Todo el conjunto de muestras (entrenamiento y validaci贸n) debe representar el fen贸meno que se est谩 aprendiendo, lo que incluye un problema de muestreo

-   La representatividad de las muestras se define en el espacio de atributos, espacial o espacio temporal (o varios espacios a la vez) seg煤n la aplicaci贸n

El sobremuestreo no afectar谩 al proceso de aprendizaje porque la suposici贸n de i.i.d. no es necesaria en el ML. Sin embargo, puede sobrestimar la precisi贸n del aprendizaje en el proceso de evaluaci贸n (Ej. Clasificaci贸n).

Una de las aplicaciones del aprendizaje por refuerzo es el muestreo espacial eficiente
:::

------------------------------------------------------------------------

### 1.2. Las caracter铆sticas espaciales

::: {style="font-size: 80%; "}
Existen varios m茅todos para incluir los componentes espaciales de los datos en la matriz de observaci贸n:

-   A帽adir directamente la referencia espacial a la matriz de datos como atributos

    -   Uno de ellos consiste en a帽adir coordenadas (por ejemplo, latitud y longitud), esto puede generar un sobreajuste considerable porque est谩n muy correlacionados

    -   A帽adir observaciones vinculadas a una regi贸n como efectos fijos de esa regi贸n (no puede no puede capturar estructuras complejas)

-   Adem谩s de la informaci贸n de referencia espacial, las entidades y los fen贸menos espaciales tienen informaci贸n intraobjeto (geom茅trica, espectral, textural y estad铆stica) y entre objetos (contextual y relacional).
:::

------------------------------------------------------------------------

### 1.3. La reducci贸n de la dimensionalidad

::: {style="font-size: 80%; "}
Un n煤mero desproporcionado de variables interrelacionadas puede afectar negativamente el aprendizaje de diversas maneras.

Existen varios m茅todos de reducci贸n de la dimensionalidad, entre ellos:

-   El an谩lisis de componentes principales (PCA)

-   El an谩lisis factorial

-   El an谩lisis de componentes independientes

-   Los mapas autoorganizados (SOM)
:::

------------------------------------------------------------------------

#### El an谩lisis de componentes principales (PCA)

::: {style="font-size: 80%; "}
El algoritmo se usa para:

-   Eliminaci贸n de caracter铆sticas

-   Extracci贸n de caracter铆sticas
:::

------------------------------------------------------------------------

#### Funcionamiento:

::: {style="font-size: 80%; "}
-   El objetivo es calcular una matriz que resuma c贸mo se relacionan todas nuestras variables entre s铆.

-   Luego dividiremos esta matriz en dos componentes separados: direcci贸n y magnitud.

![](https://miro.medium.com/max/640/1*P8_C9uk3ewpRDtevf9wVxg.png){fig-align="center"}
:::

------------------------------------------------------------------------

::: {style="font-size: 80%; "}
-   Transformaremos nuestros datos originales para alinearlos con estas direcciones importantes (que son combinaciones de nuestras variables originales).

-   Al identificar qu茅 "direcciones" son las m谩s "importantes", podemos comprimir o proyectar nuestros datos en un espacio m谩s peque帽o eliminando las "direcciones" que son las "menos importantes".

![](https://miro.medium.com/max/640/1*wsezmnzg-0N_RP3meYNXlQ.png){fig-align="center"}
:::

------------------------------------------------------------------------

#### Algoritmo

::: {style="font-size: 80%; "}
-   Tome la matriz de variables independientes $X$ y, para cada columna, reste la media de esa columna de cada entrada.

-   Si la importancia de las caracter铆sticas es independiente de la varianza de las caracter铆sticas, divida cada observaci贸n en una columna por la desviaci贸n est谩ndar de esa columna.Esta matriz es llamada $Z$.

-   Calcular la matriz de covarianzas $C = Z'Z$

-   Calcule los vectores propios y sus valores propios correspondientes a la matriz de covarianzas $C$. Usando la descomposici贸n de valores singulares *SVD* en donde se descompone como $C = LVL^T$, donde $L$ es la matriz de vectores propios y $V$ es la matriz diagonal con valores propios en la diagonal y valores de cero en cualquier otro lugar.
:::

------------------------------------------------------------------------

::: {style="font-size: 80%; "}
-   Tome los valores propios $\lambda_1, \lambda_2, \dots, \lambda_p$ p y ord茅nelos de mayor a menor. Al hacerlo, ordene los vectores propios en $L$ en consecuencia.Esta matriz ordenada de vectores propios se llama $L^*$

-   Calcula $Z^* = ZL^*$ . Esta nueva matriz, $Z^*$ , es una versi贸n centrada/estandarizada de $X$ pero ahora cada observaci贸n es una combinaci贸n de las variables originales, donde los pesos est谩n determinados por el vector propio.

![](https://miro.medium.com/max/720/1*V3JWBvxB92Uo116Bpxa3Tw.png){fig-align="center"}
:::

------------------------------------------------------------------------

#### 

::: {style="font-size: 80%; "}
El PCA puede ponderarse localmente en el espacio de los atributos (LWPCA) o en espacio geogr谩fico (GWPCA) para tener en cuenta ciertas heterogeneidades.

-   En el primer caso, LWPCA, suponemos la estructura de covarianza es homog茅nea para las observaciones que est谩n cerca unas de otras en el espacio de atributos, $_ _ _^=_$

-   El segundo, el GWPCA para la ubicaci贸n $(_, _ )$, es escrito como, $筐^ |(_, _ )=(_, _)$
:::

------------------------------------------------------------------------

### 1.4. El tratamiento de los datos faltantes

::: {style="font-size: 80%; "}
Hay diferentes maneras de abordar los valores que faltan, como: - La agregaci贸n de datos en una granularidad m谩s gruesa - La eliminaci贸n de las observaciones con valores perdidos del conjunto de datos - La imputaci贸n de valores

Los m茅todos de predicci贸n espacial siempre pueden utilizarse para imputar valores para conjuntos de datos con valores perdidos

Los enfoques m谩s conocidos para la predicci贸n espacial son los modelos estad铆sticos espaciales (por ejemplo, la regresi贸n ponderada geogr谩ficamente) y los modelos geoestad铆sticos, como kriging

El an谩lisis probabil铆stico de componentes principales (PPCA), por ejemplo, es una extensi贸n probabil铆stica de PCA y se ha utilizado para imputar valores perdidos
:::

## 2. Algoritmo de aprendizaje

::: {style="font-size: 80%;"}
En lugar de generar nuevas caracter铆sticas espaciales y procesarlas con los m茅todos tradicionales de aprendizaje de aprendizaje autom谩tico, podemos incorporar directamente las propiedades espaciales en el algoritmo de aprendizaje.

Entre todos los m茅todos de ML, los modelos m谩s utilizados son: 2.1. Los 谩rboles de decisi贸n (DT) 2.2. Los bosques aleatorios 2.3. Las m谩quinas de soporte (SVM) 2.4. Las redes neuronales 2.5. Las redes neuronales profundas (DNN)
:::

------------------------------------------------------------------------

### 2.1. Los 谩rboles de decisi贸n (DT)

#### Estructura

::: {style="font-size: 80%;"}
Un 谩rbol consta esencialmente de tres componentes principales: La ra铆z, las ramas y los nodos.

![](https://miro.medium.com/max/720/1*Ru0tIhJBdv__T0LfYxL9Gw.png){fig-align="center"}
:::

------------------------------------------------------------------------

::: {style="font-size: 80%;"}
-   Los 谩rboles de decisi贸n (DT) son m茅todos populares de ML adaptados a los problemas espaciales para superar la violaci贸n del supuesto de i.i.d. Como clase, los clasificadores de 谩rboles de decisi贸n basados en la entrop铆a espacial utilizan la ganancia de informaci贸n junto con la autocorrelaci贸n espacial para seleccionar las pruebas de los nodos de los 谩rboles candidatos en un marco espacial rasterizado.

-   Uno de los problemas m谩s frecuentes problemas que se producen en la clasificaci贸n de im谩genes mediante 谩rboles de decisi贸n es el ruido de sal y pimienta
:::

------------------------------------------------------------------------

::: {style="font-size: 80%;"}
-   El ruido de sal y pimienta se produce cuando la etiqueta predicha de un p铆xel espec铆fico difiere de sus p铆xeles vecinos y puede ser el resultado de una alta autocorrelaci贸n espacial en las etiquetas de clase de los datos de muestra utilizados para el entrenamiento

-   Jiang et al. (2003) proponen un 谩rbol de decisi贸n espacial basado en pruebas focales (FTSDT), en el que la direcci贸n de recorrido del 谩rbol de una muestra de aprendizaje se basa en las propiedades locales y focales (de vecindad) de las caracter铆sticas. Utilizan indicadores locales de asociaci贸n espacial - Lisa - como estad铆sticas de autocorrelaci贸n espacial para medir la dependencia espacial entre los p铆xeles de la vecindad.
:::

------------------------------------------------------------------------

### 2.2. Los bosques aleatorios

![](https://miro.medium.com/max/720/1*MkeRi6EMvvZvUpHmQU0dWg.webp){fig-align="center"}

------------------------------------------------------------------------

## ![](https://miro.medium.com/max/720/1*F5jHRWCpYqGhaPWCusfJhQ.png){fig-align="center"}

#### Paso a paso

::: {style="font-size: 80%;"}
-   Seleccione muestras aleatorias del conjunto de datos utilizando la agregaci贸n bootstrap.

-   Construya un 谩rbol de decisi贸n para cada muestra y guarde los resultados de predicci贸n de cada 谩rbol.

-   Calcule un voto para cada resultado predicho.

-   Determine qu茅 resultado predicho tiene m谩s votos. Esta ser谩 su predicci贸n final.
:::

------------------------------------------------------------------------

::: {style="font-size: 80%;"}
-   Se han desarrollado versiones ponderadas geogr谩ficamente de los 谩rboles de decisi贸n para para tener en cuenta la heterogeneidad espacial.

-   Qui帽ones et al. (2021) presentaron un bosque aleatorio ponderado geogr谩ficamente (GW-RF) de la diabetes tipo 2 y los factores de riesgo en los condados de Estados Unidos.

-   De forma similar a la regresi贸n ponderada geogr谩ficamente (GWR), la matriz de pesos espaciales y los bosques aleatorios (RF) se integran en un marco de an谩lisis de regresi贸n local.
:::

------------------------------------------------------------------------

### 2.3. Las m谩quinas de soporte vectorial (SVM)

::: {style="font-size: 80%;"}
-   Este m茅todo se basa en la separaci贸n de clases mediante un hiperplano (Clasificador de margen m谩ximo)

-   Las m谩quinas de soporte vectorial (SVM) se han utilizado para problemas de clasificaci贸n y regresi贸n. La idea de la SVM es mapear el espacio de entrada original a un espacio de caracter铆sticas de mayor dimensionalidad en el que las observaciones son separables por hiperplanos

-   $\xi$ se llama t茅rmino de regularizaci贸n. Los t茅rminos de regularizaci贸n se colocan en la funci贸n objetivo para controlar la complejidad del modelo y evitar el sobreajuste
:::

------------------------------------------------------------------------

![](Graficas/Fig.%203.2.png){fig-align="center"}

------------------------------------------------------------------------

::: {style="font-size: 80%;"}
驴Y si no existe ning煤n plano de separaci贸n?

![](https://miro.medium.com/max/720/1*tSLHw6cFWFUeYVeO_ahyQQ.webp){fig-align="center"}

-   En este caso, no existe un clasificador de margen m谩ximo.

-   Utilizamos un clasificador de vectores de soporte que casi puede separar las clases utilizando un margen suave llamado clasificador de vectores de soporte.
:::

------------------------------------------------------------------------

::: {style="font-size: 80%;"}
La m谩quina de vectores de soporte es una extensi贸n del clasificador de vectores de soporte que resulta de la ampliaci贸n del espacio de caracter铆sticas mediante kernels.

![](https://editor.analyticsvidhya.com/uploads/1403824.png){fig-align="center"}
:::

------------------------------------------------------------------------

![](https://miro.medium.com/max/720/1*YFvTJzdaamy1d1zT7oSMLw.webp){fig-align="center"}

------------------------------------------------------------------------

::: {style="font-size: 80%;"}
-   Lee et al. (2005) sugirieron una extensi贸n de la SVM denominada campo aleatorio de vectores de soporte que modela expl铆citamente las dependencias espaciales en la clasificaci贸n utilizando campos aleatorios condicionales (CRF).

-   El modelo contiene dos componentes: la funci贸n potencial de coincidencia de observaciones y la funci贸n potencial de consistencia local. La primera modela la relaci贸n entre las observaciones y las etiquetas de clase utilizando un clasificador SVM, y la segunda modela la relaci贸n con las etiquetas del vecindario. La funci贸n de potencial de consistencia local penaliza la discontinuidad entre los sitios pares.
:::

------------------------------------------------------------------------

### 2.4. Las redes neuronales profundas (DNN)

::: {style="font-size: 80%;"}
-   Las DNNs suelen estar compuestas por varios m贸dulos no lineales pero sencillos que representan datos a diferentes niveles.

-   Partiendo de los datos en bruto, cada m贸dulo transforma la representaci贸n de un nivel en una representaci贸n de un nivel superior (m谩s abstracto). En el proceso y utilizando el algoritmo de retropropagaci贸n (backpropagation), la m谩quina puede aprender funciones muy complejas
:::

------------------------------------------------------------------------

![](Graficas/Fig.%203.3.png){fig-align="center"}

------------------------------------------------------------------------

::: {style="font-size: 80%;"}
Las redes neuronales m谩s populares para datos espaciales pueden clasificarse a grandes rasgos en cuatro categor铆as:

-   Redes neuronales convolucionales (CNN)
-   Redes neuronales gr谩ficas profundas (GNN)
-   Redes neuronales generativas
-   Las redes neuronales recurrentes (RNN) cuando se combinan con las CNN
:::

------------------------------------------------------------------------

#### Las redes neuronales convolucionales (CNN)

::: {style="font-size: 80%;"}
-   Incluyen capas convolucionales y capas de agrupaci贸n.

-   Las capas convolucionales funcionan a partir de la convoluci贸n de una ventana deslizante (filtro) con los valores de los p铆xeles de la imagen. Los pesos del filtro se determinan autom谩ticamente a trav茅s del proceso de aprendizaje de la red.

![](Graficas/Fig.%203.4.png){fig-align="center"}
:::

------------------------------------------------------------------------

#### Arquitectura

![](https://miro.medium.com/max/720/1*uAeANQIOQPqWZnnuH-VEyw.webp){fig-align="center"}

------------------------------------------------------------------------

#### Imagen de entrada

![](https://miro.medium.com/max/640/1*15yDvGKV47a0nkf5qLKOOQ.webp){fig-align="center"}

------------------------------------------------------------------------

#### Capa de convoluci贸n - Kernel

![](https://miro.medium.com/max/640/1*GcI7G-JLAQiEoCON7xFbhg.gif){fig-align="center"}

------------------------------------------------------------------------

![](https://miro.medium.com/max/720/1*ciDgQEjViWLnCbmX-EeSrA.gif){fig-align="center"}

------------------------------------------------------------------------

#### Capa de agrupaci贸n

![](https://miro.medium.com/max/640/1*uoWYsCV5vBU8SHFPAPao-w.gif){fig-align="center"}

------------------------------------------------------------------------

![](https://miro.medium.com/max/640/1*KQIEqhxzICU7thjaQBfPBQ.webp){fig-align="center"}

------------------------------------------------------------------------

![](https://miro.medium.com/max/720/1*uAeANQIOQPqWZnnuH-VEyw.webp){fig-align="center"}

------------------------------------------------------------------------

::: {style="font-size: 80%;"}
-   Esta caracter铆stica de las CNN se ha utilizado de forma limitada para definir autom谩ticamente los pesos de la matriz W en otras aplicaciones espaciales.

-   Las capas de pooling agregan los p铆xeles vecinos en un 煤nico p铆xel, reduciendo las dimensiones totales de la imagen.

-   A medida que aumenta el n煤mero de capas convolucionales y de pooling, la red se vuelve m谩s profunda y puede extraer caracter铆sticas m谩s abstractas. Una red de clasificaci贸n sigue finalmente el procesamiento de estas capas.
:::

------------------------------------------------------------------------

### 2.5. Precisi贸n espacial

::: {style="font-size: 80%;"}
-   Las medidas de la precisi贸n de los datos espaciales son esenciales para ser consideradas en la funci贸n objetivo en el aprendizaje autom谩tico para una serie de aplicaciones geoespaciales.

-   Cuando las variables est谩n rasterizadas, una forma t铆pica de evaluar la precisi贸n de la predicci贸n es medir la similitud entre los mapas predichos y los reales.

$$F = Similaridad(\text{Presici贸n de la Clasificaci贸n})$$
:::

------------------------------------------------------------------------

::: {style="font-size: 80%;"}
-   Los autores sugieren a帽adir un t茅rmino a la funci贸n objetivo para medir la precisi贸n espacial precisi贸n espacial, que podr铆a ser la distancia media a la predicci贸n m谩s cercana. Por lo tanto, podemos reescribir la funci贸n objetivo como sigue:

$$F = Similaridad[(1 - \alpha)\text{Presici贸n de la Clasificaci贸n} + \alpha \text{Presici贸n espacial}]$$ donde $\alpha$ es un par谩metro regularizador que se ajusta durante el entrenamiento.

![](Graficas/Fig.%203.5.png){fig-align="center"}
:::

------------------------------------------------------------------------

### 2.6. Generalizaci贸n

::: {style="font-size: 80%;"}
-   La generalizaci贸n es la capacidad de generalizar un modelo entrenado basado en un conjunto de datos a conjuntos de datos futuros.

-   Un conjunto de datos suele dividirse en tres conjuntos mutuamente excluyentes:

    -   Un conjunto de entrenamiento
    -   Un conjunto de validaci贸n
    -   Un conjunto de prueba

-   En cada iteraci贸n ajustamos un modelo a nuestro conjunto de datos de entrenamiento y calculamos una funci贸n objetivo para medir el rendimiento del aprendizaje.
:::

------------------------------------------------------------------------

::: {style="font-size: 80%;"}
-   Se utiliza el conjunto de datos de validaci贸n para evaluar la capacidad del modelo para ajustarse a otro conjunto de datos para la generalizaci贸n.

-   Con el conjunto de datos de prueba se pueden utilizar m茅todos de validaci贸n m谩s complejos, como la validaci贸n cruzada k-fold, para garantizar el mejor ajuste de los conjuntos de datos de entrenamiento, prueba y validaci贸n.
:::

# Aprendizaje espacio - tiemporal {background-color="#581845"}

## Las redes convolucionales de memoria a corto plazo (LSTM)

::: {style="font-size: 80%;"}
-   Las redes neuronales recurrentes convolucionales (RNN) y, en especial, las redes convolucionales de memoria a corto plazo (LSTM) pueden aplicarse ampliamente al aprendizaje espacio-temporal de los datos de la red datos.

-   LSTM es un tipo de red neuronal recurrente con la capacidad de memorizar dependencias temporales en los datos. La combinaci贸n de esta caracter铆stica con el poder de las CNN para para aprender las caracter铆sticas espaciales jer谩rquicas puede proporcionar un modelo autom谩tico de ML de un solo paso para para tener en cuenta la dependencia espacio-temporal
:::

------------------------------------------------------------------------

### Red Neuronal Recurente (RNN) y LSTM

::: {style="font-size: 80%;"}
-   Las redes neuronales recurrentes sufren de memoria a corto plazo. Si una secuencia es lo suficientemente larga, tendr谩n dificultades para llevar informaci贸n de pasos de tiempo anteriores a los posteriores.

-   LSTM se creo como soluci贸n a la memoria a corto plazo. Tienen mecanismos internos llamados puertas que pueden regular el flujo de informaci贸n.

-   Estas puertas pueden aprender qu茅 datos en una secuencia es importante conservar o desechar. Al hacerlo, puede pasar informaci贸n relevante a lo largo de la larga cadena de secuencias para hacer predicciones.
:::

------------------------------------------------------------------------

::: {style="font-size: 80%;"}
Un RNN funciona as铆; Las primeras palabras se transforman en vectores legibles por m谩quina. Luego, la RNN procesa la secuencia de vectores uno por uno.

![](https://miro.medium.com/max/720/1*AQ52bwW55GsJt6HTxPDuMA.gif){fig-align="center" width="900" height="325"}
:::

------------------------------------------------------------------------

::: {style="font-size: 80%;"}
Pasa el estado oculto anterior al siguiente paso de la secuencia. El estado oculto act煤a como la memoria de las redes neuronales.

![](https://miro.medium.com/max/1400/1*o-Cq5U8-tfa1_ve2Pf3nfg.gif){fig-align="center"}
:::

------------------------------------------------------------------------

::: {style="font-size: 80%;"}
Primero, la entrada y el estado oculto anterior se combinan para formar un vector. Ese vector ahora tiene informaci贸n sobre la entrada actual y las entradas anteriores. El vector pasa por la activaci贸n de tanh y la salida es el nuevo estado oculto, o la memoria de la red.

![](https://miro.medium.com/max/1400/1*WMnFSJHzOloFlJHU6fVN-g.gif){fig-align="center"}
:::

------------------------------------------------------------------------

::: {style="font-size: 80%;"}
La activaci贸n de tanh se utiliza para ayudar a regular los valores que fluyen a trav茅s de la red. La funci贸n tanh reduce los valores para que siempre est茅n entre -1 y 1.

![](https://miro.medium.com/max/1400/1*iRlEg1GBKRzGTre5aOQUCg.gif){fig-align="center" width="550" height="250"}

![](https://miro.medium.com/max/1400/1*gFC2bTg3uihp1klknWU0qg.gif){fig-align="center"}
:::

------------------------------------------------------------------------

#### LSTM

::: {style="font-size: 80%;"}
Un LSTM tiene un flujo de control similar al de una red neuronal recurrente. Procesa los datos que transmiten informaci贸n a medida que se propaga hacia adelante. Las diferencias son las operaciones dentro de las c茅lulas del LSTM.

Estas operaciones se utilizan para permitir que el LSTM conserve u olvide informaci贸n.

![](https://miro.medium.com/max/1400/1*0f8r3Vd-i4ueYND1CUrhMA.webp){fig-align="center" width="700" height="400"}
:::

------------------------------------------------------------------------

#### Sigmoideo

::: {style="font-size: 80%;"}
-   Gates contiene activaciones sigmoideas. Una activaci贸n sigmoidea es similar a la activaci贸n de tanh. En lugar de aplastar los valores entre -1 y 1, aplasta los valores entre 0 y 1. Eso es 煤til para actualizar u olvidar datos porque cualquier n煤mero que se multiplique por 0 es 0, lo que hace que los valores desaparezcan o se "olviden".

![](https://miro.medium.com/max/1400/1*rOFozAke2DX5BmsX2ubovw.gif){fig-align="center"}
:::

------------------------------------------------------------------------

#### Puerta de olvido

::: {style="font-size: 80%;"}
Primero, tenemos la puerta del olvido. Esta puerta decide qu茅 informaci贸n debe desecharse o conservarse. La informaci贸n del estado oculto anterior y la informaci贸n de la entrada actual se pasan a trav茅s de la funci贸n sigmoidea.

![](https://miro.medium.com/max/1400/1*GjehOa513_BgpDDP6Vkw2Q.gif){fig-align="center"}
:::

------------------------------------------------------------------------

#### Puerta de entrada

::: {style="font-size: 80%;"}
Para actualizar el estado de la celda, tenemos la puerta de entrada. Primero, pasamos el estado oculto anterior y la entrada actual a una funci贸n sigmoidea.

Tambi茅n pasa el estado oculto y la entrada actual a la funci贸n tanh para reducir los valores entre -1 y 1 para ayudar a regular la red.

Luego, multiplica la salida de tanh con la salida sigmoidea. La salida sigmoide decidir谩 qu茅 informaci贸n es importante mantener de la salida tanh.

![](https://miro.medium.com/max/1400/1*TTmYy7Sy8uUXxUXfzmoKbA.gif){fig-align="center" width="700" height="350"}
:::

------------------------------------------------------------------------

#### Estado de la celda

::: {style="font-size: 80%;"}
Ahora deber铆amos tener suficiente informaci贸n para calcular el estado de la celda. Primero, el estado de la celda se multiplica puntualmente por el vector de olvido.

Luego tomamos la salida de la puerta de entrada y hacemos una suma puntual que actualiza el estado de la celda a nuevos valores que la red neuronal considera relevantes. Eso nos da nuestro nuevo estado de la celda.

![](https://miro.medium.com/max/1400/1*S0rXIeO_VoUVOyrYHckUWg.gif){fig-align="center" width="700" height="350"}
:::

------------------------------------------------------------------------

#### Puerta de salida

::: {style="font-size: 80%;"}
Por 煤ltimo tenemos la puerta de salida. La puerta de salida decide cu谩l deber铆a ser el siguiente estado oculto.

Primero, pasamos el estado oculto anterior y la entrada actual a una funci贸n sigmoidea. Luego pasamos el estado de celda reci茅n modificado a la funci贸n tanh. Multiplicamos la salida de tanh con la salida sigmoide para decidir qu茅 informaci贸n debe llevar el estado oculto. La salida es el estado oculto. El nuevo estado de celda y el nuevo oculto se transfieren al siguiente paso de tiempo.

![](https://miro.medium.com/max/1400/1*VOXRGhOShoWWks6ouoDN3Q.gif){fig-align="center" width="700" height="340"}
:::

# Gracias {background-color="#581845"}
